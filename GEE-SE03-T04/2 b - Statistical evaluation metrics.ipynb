{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ecee7ffa-ec8b-4adb-91ab-475b090947c1",
      "metadata": {
        "editable": true
      },
      "source": "![](./figures/Logo.PNG)"
    },
    {
      "cell_type": "markdown",
      "id": "49ae7cf3-5f1c-4c4b-ac38-adaeb9ade587",
      "metadata": {
        "editable": true
      },
      "source": "## In this part of the tutorial, you will\n* use metrics to assess simulation performance\n* study scatter plots of multiple objective functions"
    },
    {
      "cell_type": "markdown",
      "id": "e21dc5cf-4d8e-4f8e-a126-9dda81843905",
      "metadata": {
        "editable": true
      },
      "source": "- - -"
    },
    {
      "cell_type": "markdown",
      "id": "ca4b0d95-c290-4a55-8f0c-f879ef6b2a56",
      "metadata": {
        "editable": true
      },
      "source": "# 2b - Statistical Evaluation Metrics"
    },
    {
      "cell_type": "markdown",
      "id": "fcfb4367-5461-4333-928c-a838665bb073",
      "metadata": {
        "editable": true
      },
      "source": "- - -"
    },
    {
      "cell_type": "markdown",
      "id": "e38f84d9-bd20-41ad-bc47-afc5b7d7b119",
      "metadata": {
        "editable": true
      },
      "source": "## 1. Introducing Bias, RMSE, NSE and KGE"
    },
    {
      "cell_type": "markdown",
      "id": "a3706633-d527-4e26-b24b-ca3a5e497878",
      "metadata": {
        "editable": true
      },
      "source": "In tutorial 2a, we have relied on visual inspection to learn about the model performance and to fit of the model output to the observed runoff. For some sets of parameter combinations, it can be difficult to assess which set returns the best result. In this tutorial, we will use evaluation metrics, enabling a more robust comparison between model runs with different parameterizations."
    },
    {
      "cell_type": "markdown",
      "id": "3ef81497-9421-49b3-a4d5-5d53d0423b37",
      "metadata": {
        "editable": true
      },
      "source": "We start with a simple metric of closeness: the **bias**.\n\n**Bias**: Bias is the consistent deviation of simulation results from observed values. It indicates the model's tendency to systematically overestimate or underestimate the target variable.\n\nLet $y_i$ represent the observed value and $\\hat{y}_i$ denote the simulated value. The bias is calculated as:\n\n$$\n\\text{Bias} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n$$\n\nwhere $n$ is the total number of data points."
    },
    {
      "cell_type": "markdown",
      "id": "25ca25f0-3a5b-4be6-9da8-ee63c8587fe4",
      "metadata": {
        "editable": true
      },
      "source": "Next, we will look at three additional metrics: **Root Mean Square Error (RMSE)**, **Kling-Gupta Efficiency (KGE)**, and **Nash-Sutcliffe Efficiency (NSE)**.\n\n**Root Mean Square Error (RMSE)**: RMSE measures the square root of the average squared differences between predicted values and the corresponding actual values (in other words: the square root of the MSE).\n\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n$$\n\nwhere $y_i$ represents the observed value, $\\hat{y}_i$ denotes the simulated value, and $n$ is the total number of data points.\n\n**Kling-Gupta Efficiency (KGE)**: KGE is a hydrological metric that assesses the performance of hydrological models by measuring the correlation, bias, and variability of their predictions against observed hydrograph data. It allows evaluation of the model's accuracy, timing, and volume representation.\n\n$$\n\\text{KGE} = 1 - \\sqrt{(r - 1)^2 + (\\alpha - 1)^2 + (\\beta - 1)^2}\n$$\n\nwhere $r$ represents the Pearson correlation coefficient, $\\alpha$ (alpha) is the ratio of the standard deviations between observed and simulated values, and $\\beta$ (beta) is the ratio of their means.\n\n**Nash-Sutcliffe Efficiency (NSE)**: NSE measures the proportion of the observed variance that is explained by the model results. It is particularly useful for evaluating streamflow predictions. A perfect NSE value of 1 indicates a perfect fit between the model and observed data, while negative values suggest the model performs worse than simply using the mean of the observed values.\n\n$$\n\\text{NSE} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n$$\n\nwhere $y_i$ represents the observed value, $\\hat{y}_i$ denotes the simulated value, $n$ is the total number of data points, and $\\bar{y}$ is the mean of the observed values."
    },
    {
      "cell_type": "markdown",
      "id": "15540fcc-8bcd-41fd-a94e-30af5b88ea0b",
      "metadata": {
        "editable": true
      },
      "source": "<div style=\"background:#e0f2fe; padding: 1%; border: 1mm solid SkyBlue\">\n    <h4><span>&#129300 </span>Your Turn I: Understanding the Metrics</h4>\n    <ol>\n        <li>Can you think of a reason why one would prefer RMSE to MSE?</li>\n        <li>What are likely limitations of such metrics? What do they miss?</li>\n    </ol>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "628f0f7e-9181-4451-8b6c-fd5c80225673",
      "metadata": {
        "editable": true
      },
      "source": "## 2. Using Bias, RMSE, NSE and KGE"
    },
    {
      "cell_type": "markdown",
      "id": "ea0f755f-e94e-4189-affe-e54114150c7d",
      "metadata": {
        "editable": true
      },
      "source": "**Import packages**"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3b214a76-ad32-4776-b819-4db5533d0552",
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport scipy\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdate\nimport sys\nsys.path.append('src/')\nimport HyMod\nfrom ipywidgets import interact, Dropdown "
    },
    {
      "cell_type": "markdown",
      "id": "49142c51-d6c5-4cfa-bc2c-58bb0d5d195e",
      "metadata": {
        "editable": true
      },
      "source": "**Defining Bias, RMSE, NSE and KGE**\n\nBy the way, the red string at the start of the function which uses three  <code style=\"color:darkred\">\"\"\"</code> is called [docstring](https://realpython.com/documenting-python-code/#documenting-your-python-code-base-using-docstrings). It acts as a description of the function and is also used to describe the arguments and return value. We would suggest that you write docstring whenever the function gets more complicated or its arguments aren't immediately clear.\n\nIn Jupyter Notebook (or Lab) the documentation can be accessed by pressing `Shift` + `Tab` (both Windows and Mac) when the cursor is placed in the function call. This also works for all other functions and modules, e.g. _numpy_, _scipy_, _matplotlib_, ..."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31254d83-2fa2-4add-9a28-418c8fb422d8",
      "metadata": {},
      "outputs": [],
      "source": "def bias(obs, sim):\n    \"\"\"\n    Calculate the Bias between observed and simulated values.\n    \n    Bias measures the consistent deviation of simulation results from observed values,\n    indicating whether the model systematically overestimates or underestimates the target variable.\n    \"\"\"\n    return np.mean(np.subtract(obs, sim))  # Mean of observation values minus simulation results\n\ndef rmse(obs, sim):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between observed and simulated values.\n    \n    RMSE measures the square root of the average squared differences between predicted and actual values.\n    \"\"\"\n    return np.sqrt(np.mean(np.square(np.subtract(obs, sim))))\n\ndef nse(obs, sim):\n    \"\"\"\n    Calculate the Nash-Sutcliffe Efficiency (NSE) between observed and simulated values.\n    \n    NSE measures the proportion of the observed variance explained by the model results.\n    A perfect NSE of 1 indicates a perfect fit, while negative values suggest worse performance than using the mean of observed values.\n    \"\"\"\n    r_nse = np.corrcoef(obs, sim)[0][1] \n    alpha_nse = np.divide(np.std(sim), np.std(obs))\n    beta_nse = np.divide(np.subtract(np.mean(sim), np.mean(obs)), np.std(obs))\n    nse = 2 * alpha_nse * r_nse - np.square(alpha_nse) - np.square(beta_nse)\n    return nse\n\ndef kge(obs, sim):\n    \"\"\"\n    Calculate the Kling-Gupta Efficiency (KGE) between observed and simulated values.\n    \n    KGE assesses model performance by measuring correlation, bias, and variability against observed data.\n\n    Returns:\n    tuple: (correlation coefficient, variation ratio, bias ratio, KGE value)\n    \"\"\"\n    r_kge = np.corrcoef(obs, sim)[0][1]  # Pearson correlation coefficient\n    alpha_kge = np.divide(np.std(sim), np.std(obs))  # Variation ratio\n    beta_kge = np.divide(np.mean(sim), np.mean(obs))  # Bias\n    kge = 1 - np.sqrt(np.square(r_kge - 1) + np.square(beta_kge - 1) + np.square(alpha_kge - 1))\n    return round(r_kge, 3), round(alpha_kge, 3), round(beta_kge, 3), round(kge, 3)\n\ndef kge_only(obs, sim):\n    \"\"\"\n    Calculate the Kling-Gupta Efficiency (KGE) between observed and simulated values.\n    \n    Returns only the KGE value, excluding intermediate metrics.\n    \"\"\"\n    _, _, _, kge_value = kge(obs, sim)\n    return kge_value"
    },
    {
      "cell_type": "markdown",
      "id": "e21b57d4-8893-49cc-bf0b-cb8257878bb1",
      "metadata": {
        "editable": true
      },
      "source": "**Create and display dropdown for selecting catchment**"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b0d6535d-6918-4af8-a91a-3acba1bda0f1",
      "metadata": {
        "editable": true,
        "jupyter": {
          "source_hidden": false
        },
        "source_hidden": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11307f0b05c44a10ae1fa4b0be56fec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Dropdown(description='Catchment:', options=('Trout River, BC, Canada', 'Medina River, TX, USA', 'Siletz River,\u2026"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": "# DO NOT ALTER! code to select the catchment\n\ncatchment_names = [\"Trout River, BC, Canada\", \"Medina River, TX, USA\", \"Siletz River, OR, USA\"]\ndropdown = Dropdown(\n    options=catchment_names,\n    value=catchment_names[0],\n    description='Catchment:',\n    disabled=False)\n\ndisplay(dropdown)"
    },
    {
      "cell_type": "markdown",
      "id": "802d3738-d190-420a-9431-b117e0a5d4ee",
      "metadata": {
        "editable": true
      },
      "source": "**Read catchment data and prepare input data for model**"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9d527fac-e269-4d49-a094-dda5b0e9611d",
      "metadata": {},
      "outputs": [],
      "source": "# Read catchment data\ncatchment_name = dropdown.value\n# Read catchment data\nfile_dic = {catchment_names[0]: \"hysets_10BE007\", catchment_names[1]: \"camels_08178880\", catchment_names[2]: \"camels_14305500\"}\ndf_obs = pd.read_csv(f\"data/{file_dic[catchment_name]}.csv\")\n# Make sure the date is interpreted as a datetime object -> makes temporal operations easier\ndf_obs.date = pd.to_datetime(df_obs['date'], format='%Y-%m-%d')\n# Select time frame\nstart_date = '2002-10-01'\nend_date = '2003-09-30'\n\n# Index frame by date\ndf_obs.set_index('date', inplace=True)\n# Select time frame\ndf_obs = df_obs[start_date:end_date]\n# Reformat the date for plotting\ndf_obs[\"date\"] = df_obs.index.map(lambda s: s.strftime('%b-%d-%y'))\n# Reindex\ndf_obs = df_obs.reset_index(drop=True)\n# Select snow, precip, PET, streamflow and T\ndf_obs = df_obs[[\"snow_depth_water_equivalent_mean\", \"total_precipitation_sum\",\"potential_evaporation_sum\",\"streamflow\", \"temperature_2m_mean\", \"date\"]]\n# Rename variables\ndf_obs.columns = [\"Snow [mm/day]\", \"P [mm/day]\", \"PET [mm/day]\", \"Q [mm/day]\", \"T [C]\", \"Date\"]\n\n# Prepare the data intput for both models\nP = df_obs[\"P [mm/day]\"].to_numpy()\nevap = df_obs[\"PET [mm/day]\"].to_numpy()\ntemp = df_obs[\"T [C]\"].to_numpy()    "
    },
    {
      "cell_type": "markdown",
      "id": "538e390b-d4f8-4026-aed4-8a20b2a15e63",
      "metadata": {
        "editable": true
      },
      "source": "**Using bias and RMSE to evaluate HyMOD results**"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a69fff3f-531b-4d64-a2a5-eecd1c091d82",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "988d6d1c80314ba5903b9cc86735e095",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "interactive(children=(IntSlider(value=200, description='Sm_blue', max=400), FloatSlider(value=1.0, description\u2026"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": "@interact(\n    Sm_blue=(0, 400, 1), beta_blue=(0, 2, 0.01), alfa_blue=(0, 1, 0.01), Rs_blue=(8.0, 200.0, 0.5), Rf_blue=(1.0, 7.0, 0.5),\n    Sm_red=(0, 400, 1), beta_red=(0, 2, 0.01), alfa_red=(0, 1, 0.01), Rs_red=(8.0, 200.0, 0.5), Rf_red=(1.0, 7.0, 0.5), \n    lmbda = (0.01, 1.0, 0.01)\n)\ndef compare_hymod_runs(Sm_blue=200, beta_blue=1, alfa_blue=0.5, Rs_blue=50, Rf_blue=6, \n                       Sm_red=150, beta_red=0.7, alfa_red=0.7, Rs_red=40, Rf_red=3,\n                       lmbda=0.2):\n    # Run HyMOD simulation 1 (blue)\n    param_blue = np.array([Sm_blue, beta_blue, alfa_blue, 1/Rs_blue, 1/Rf_blue])\n    q_sim_blue, states_blue, fluxes_blue = HyMod.hymod_sim(param_blue, P, evap)\n    # Make Dataframe from results\n    df_model_blue = pd.DataFrame({'Q [mm/day]': q_sim_blue, 'ET [mm/day]': fluxes_blue.T[0], 'Date': df_obs[\"Date\"].to_numpy()})\n\n    # Run HyMOD simulation 2 (red)\n    param_red = np.array([Sm_red, beta_red, alfa_red, 1/Rs_red, 1/Rf_red])\n    q_sim_red, states_red, fluxes_red = HyMod.hymod_sim(param_red, P, evap)\n    # Make Dataframe from results\n    df_model_red = pd.DataFrame({'Q [mm/day]': q_sim_red, 'ET [mm/day]': fluxes_red.T[0], 'Date': df_obs[\"Date\"].to_numpy()})\n\n    # The Box-Cox transform is given by:\n    # y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n    #     log(x),                  for lmbda = 0\n    # reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n    df_obs[\"BC-Q [mm/day]\"] = scipy.stats.boxcox(df_obs['Q [mm/day]'].array, lmbda=lmbda)\n    df_model_red[\"BC-Q [mm/day]\"] = scipy.stats.boxcox(df_model_red['Q [mm/day]'].array, lmbda=lmbda)\n    df_model_blue[\"BC-Q [mm/day]\"] = scipy.stats.boxcox(df_model_blue['Q [mm/day]'].array, lmbda=lmbda)\n    \n    # Plot results\n    plt.close()\n    fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n\n    # Plot the simulated and observed Q\n    sns.lineplot(data=df_model_blue, x=\"Date\", y=\"Q [mm/day]\", label=\"HyMOD (blue)\", color=\"blue\")\n    sns.lineplot(data=df_model_red, x=\"Date\", y=\"Q [mm/day]\", label=\"HyMOD (red)\", color=\"red\")\n    sns.lineplot(data=df_obs, x=\"Date\", y=\"Q [mm/day]\", color=\"black\", label=\"Observed\")\n\n    # Show only the main ticks\n    locator = mdate.MonthLocator()\n    plt.gca().xaxis.set_major_locator(locator)\n    ax.set_title(catchment_name)\n    \n    # Display the figure\n    plt.show()\n\n    # Calculate metrics\n    bias_blue = round(bias(df_obs[\"Q [mm/day]\"], df_model_blue[\"Q [mm/day]\"]), 3)\n    rmse_blue = round(rmse(df_obs[\"Q [mm/day]\"], df_model_blue[\"Q [mm/day]\"]), 3)\n    bc_rmse_blue = round(rmse(df_obs[\"BC-Q [mm/day]\"], df_model_blue[\"BC-Q [mm/day]\"]), 3)  \n\n    bias_red = round(bias(df_obs[\"Q [mm/day]\"], df_model_red[\"Q [mm/day]\"]), 3)\n    rmse_red = round(rmse(df_obs[\"Q [mm/day]\"], df_model_red[\"Q [mm/day]\"]), 3)\n    bc_rmse_red = round(rmse(df_obs[\"BC-Q [mm/day]\"], df_model_red[\"BC-Q [mm/day]\"]), 3)\n\n    # Print metric results\n    print(f\"Blue model run: Bias = {bias_blue}, RMSE = {rmse_blue}, B.C.-RMSE = {bc_rmse_blue}\")\n    print(f\"Red  model run: Bias = {bias_red}, RMSE = {rmse_red}, B.C.-RMSE = {bc_rmse_red}\")"
    },
    {
      "cell_type": "markdown",
      "id": "ee9e5d45-1d05-4122-8a9b-8f49deb86166",
      "metadata": {
        "editable": true
      },
      "source": "<div style=\"background:#e0f2fe; padding:1%; border: 1mm solid SkyBlue\">\n    <h4><span>&#129300 </span>Your Turn II: Apply and Discuss these Metrics</h4>\n    <ol>\n        <li>Describe the model performance evaluated with RMSE and Box-Cox transformed RMSE.</li>\n        <li>What do the metrics capture? What do they miss?</li>\n        <li>What value does additionally assessing the bias have?</li>\n    </ol>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "4416c4d0-d766-4261-a73f-6dc17555f0ce",
      "metadata": {
        "editable": true
      },
      "source": "**Multiple objectives with bias and RMSE**"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "00a85e74-7cde-487e-a3de-9933554ed4e3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47e7d8d1430d4876a1fcae51ab3c6148",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "interactive(children=(FloatSlider(value=0.2, description='lmbda', max=1.0, min=0.01, step=0.01), Output()), _d\u2026"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": "@interact(lmbda = (0.01, 1.0, 0.01))    \ndef multiple_objectives(lmbda=0.2):\n    # Define bounds for the parameters\n    bounds = {\"Sm\": (0, 400), \"beta\": (0, 2), \"alfa\": (0, 1), \"Rs\": (8, 200), \"Rf\": (1, 7)}\n    \n    # Create random parameter sets within the given ranges\n    n = 200\n    mult = n  # multiplier for integer generation, after which the integers are divided by mult to get the samples\n    samples_Sm = np.array(random.sample(range(bounds[\"Sm\"][0]*mult, bounds[\"Sm\"][1]*mult), n)) / mult  # generate array of random samples within bounds\n    samples_beta = np.array(random.sample(range(bounds[\"beta\"][0]*mult, bounds[\"beta\"][1]*mult), n)) / mult\n    samples_alfa = np.array(random.sample(range(bounds[\"alfa\"][0]*mult, bounds[\"alfa\"][1]*mult), n)) / mult\n    samples_Rs = np.array(random.sample(range(bounds[\"Rs\"][0]*mult, bounds[\"Rs\"][1]*mult), n)) / mult\n    samples_Rf = np.array(random.sample(range(bounds[\"Rf\"][0]*mult, bounds[\"Rf\"][1]*mult), n)) / mult\n    parameter_sets = np.column_stack((samples_Sm, samples_beta, samples_alfa, samples_Rs, samples_Rf))  # stack the generates samples as columns\n    \n    df_obs[\"BC-Q [mm/day]\"] = scipy.stats.boxcox(df_obs['Q [mm/day]'].array, lmbda=lmbda)  # Box-Cox transform observations\n    \n    # Run HyMod for all the parameter sets and compute the metrics\n    l_bias = []\n    l_rmse = []\n    l_bc_rmse = []\n    for set_id, parameter_set in enumerate(parameter_sets):\n        # Run HyMod, and get the ouput\n        Sm, beta, alfa, Rs, Rf = parameter_set\n        runoff_sim, states, fluxes = HyMod.hymod_sim([Sm, beta, alfa, 1/Rs, 1/Rf], P, evap)\n        df_model = pd.DataFrame({'Q [mm/day]': runoff_sim[-365:], 'ET [mm/day]': fluxes.T[0][-365:], 'Date': df_obs[\"Date\"].to_numpy()})\n        l_bias.append(abs(round(bias(df_obs[\"Q [mm/day]\"], df_model[\"Q [mm/day]\"]), 3)))  # calculate bias, round result and append to list\n        l_rmse.append(round(rmse(df_obs[\"Q [mm/day]\"], df_model[\"Q [mm/day]\"]), 3))  # calculate rmse, round result and append to list \n        \n        df_model[\"BC-Q [mm/day]\"] = scipy.stats.boxcox(df_model['Q [mm/day]'].array, lmbda=lmbda)  # Box-Cox transform simulation results\n        l_bc_rmse.append(round(rmse(df_obs[\"BC-Q [mm/day]\"], df_model[\"BC-Q [mm/day]\"]), 3))  # calculate bc rmse, round result and append to list\n\n    df_results = pd.DataFrame({'abs(Bias)': l_bias, 'RMSE': l_rmse, 'B.C.-RMSE': l_bc_rmse})  # create dataframe from metric results\n    \n    # Plot results\n    plt.close()\n    fig, axes = plt.subplots(1, 3, figsize=(20, 4))  # create three subplots\n    sns.scatterplot(df_results, x='RMSE', y='abs(Bias)', ax=axes[0])\n    sns.scatterplot(df_results, x='B.C.-RMSE', y='abs(Bias)', ax=axes[1])\n    sns.scatterplot(df_results, x='RMSE', y='B.C.-RMSE', ax=axes[2])\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "id": "820f7bdd-4895-4ab9-9442-945be90570ba",
      "metadata": {
        "editable": true
      },
      "source": "**NSE, KGE and their common parts**"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71f33702-1cff-41e1-ba12-b36ce27d78f9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea4192a5df247198b82d77a84293649",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "interactive(children=(IntSlider(value=200, description='Sm_blue', max=400), FloatSlider(value=1.0, description\u2026"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": "@interact(\n    Sm_blue=(0, 400, 1), beta_blue=(0, 2, 0.01), alfa_blue=(0, 1, 0.01), Rs_blue=(8.0, 200.0, 0.5), Rf_blue=(1.0, 7.0, 0.5),\n    Sm_red=(0, 400, 1), beta_red=(0, 2, 0.01), alfa_red=(0, 1, 0.01), Rs_red=(8.0, 200.0, 0.5), Rf_red=(1.0, 7.0, 0.5)\n)    \ndef compare_hymod_runs(Sm_blue=200, beta_blue=1, alfa_blue=0.5, Rs_blue=50, Rf_blue=6, \n                       Sm_red=150, beta_red=0.7, alfa_red=0.7, Rs_red=40, Rf_red=3):\n    # Run HyMOD simulation 1 (blue)\n    param_blue = np.array([Sm_blue, beta_blue, alfa_blue, 1/Rs_blue, 1/Rf_blue])\n    q_sim_blue, states_blue, fluxes_blue = HyMod.hymod_sim(param_blue, P, evap)\n    df_model_blue = pd.DataFrame({'Q [mm/day]': q_sim_blue[-365:], 'ET [mm/day]': fluxes_blue.T[0][-365:], 'Date': df_obs[\"Date\"].to_numpy()})\n\n    # Run HyMOD simulation 2 (red)\n    param_red = np.array([Sm_red, beta_red, alfa_red, 1/Rs_red, 1/Rf_red])\n    q_sim_red, states_red, fluxes_red = HyMod.hymod_sim(param_red, P, evap)\n    df_model_red = pd.DataFrame({'Q [mm/day]': q_sim_red[-365:], 'ET [mm/day]': fluxes_red.T[0][-365:], 'Date': df_obs[\"Date\"].to_numpy()})\n    \n    # Plot results\n    plt.close()\n    fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n\n    # Plot the simulated and observed Q\n    sns.lineplot(data=df_model_blue, x=\"Date\", y=\"Q [mm/day]\", label=\"HyMOD (blue)\", color=\"blue\")\n    sns.lineplot(data=df_model_red, x=\"Date\", y=\"Q [mm/day]\", label=\"HyMOD (red)\", color=\"red\")\n    sns.lineplot(data=df_obs, x=\"Date\", y=\"Q [mm/day]\", color=\"black\", label=\"Observed\")\n\n    # Show only the main ticks\n    locator = mdate.MonthLocator()\n    plt.gca().xaxis.set_major_locator(locator)\n    ax.set_title(catchment_name)\n    \n    # Display the figure\n    plt.show()\n\n    # Calculate metrics\n    nse_blue = round(nse(df_obs[\"Q [mm/day]\"], df_model_blue[\"Q [mm/day]\"]), 3)\n    r_kge_blue, alpha_kge_blue, beta_kge_blue, kge_blue = kge(df_obs[\"Q [mm/day]\"], df_model_blue[\"Q [mm/day]\"])\n\n    nse_red = round(nse(df_obs[\"Q [mm/day]\"], df_model_red[\"Q [mm/day]\"]), 3)\n    r_kge_red, alpha_kge_red, beta_kge_red, kge_red = kge(df_obs[\"Q [mm/day]\"], df_model_red[\"Q [mm/day]\"])\n\n    # Print metric results\n    print(f\"Blue model run: NSE = {nse_blue}, KGE = {kge_blue}, (KGE components: r = {r_kge_blue}, alpha = {alpha_kge_blue}, beta = {beta_kge_blue})\")\n    print(f\"Red  model run: NSE = {nse_red}, KGE = {kge_red}, (KGE components: r = {r_kge_red}, alpha = {alpha_kge_red}, beta = {beta_kge_red})\")"
    },
    {
      "cell_type": "markdown",
      "id": "339afca0-dee9-4434-9364-a11f8db89ad1",
      "metadata": {
        "editable": true
      },
      "source": "<div style=\"background:#e0f2fe; padding:1%; border: 1mm solid SkyBlue\">\n    <h4><span>&#129300 </span>Your Turn III: Comparing NSE and KGE</h4>\n    <ol>\n        <li>Compare the values of NSE and KGE! Under what circumstances do either of them give a result closer to their common optimal value (which is 1).</li>\n        <li>Which component of KGE dominates the result? (reminder: in KGE, r represents the correlation coefficient, alpha is the ratio of the standard deviations between observed and simulated values, and beta is the ratio of their means)</li>\n    </ol>\n    Additional Assignments\n    <ol start=3>\n        <li>Create a multi-objective scatter plot with NSE and KGE on the x- and y-axis (similarly to the scatter plots in Exercise 3 above).</li>\n        <li>Create a new function (RMEE - \"root mean exponent error\") that alters the RMSE function: replace the square by an integer exponent that is also applied to the root. Implement its use and make a scatter plot of RMEE and RMSE.</li>\n    </ol>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "62a50a76-b4e9-4680-a1ff-427946031e0f",
      "metadata": {
        "editable": true
      },
      "source": "**Multiple objectives with NSE and KGE**"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7865f1c1-f42f-42bb-9a5e-f8d77a1ded2a",
      "metadata": {},
      "outputs": [],
      "source": "# implement your code here (you may of course copy and paste parts of the python cell above)\n# beware that you will not need to run this cell interactively => remove the lines with \"@interact...\" and \"def...\"\n# you also will not need the Box-Cox transformation"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
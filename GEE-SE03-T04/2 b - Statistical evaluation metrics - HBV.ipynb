{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecee7ffa-ec8b-4adb-91ab-475b090947c1",
   "metadata": {
    "editable": false
   },
   "source": [
    "![](./figures/Logo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae7cf3-5f1c-4c4b-ac38-adaeb9ade587",
   "metadata": {
    "editable": false
   },
   "source": [
    "## In this part of the tutorial, you will\n",
    "* use metrics to assess simulation performance\n",
    "* study scatter plots of multiple objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21dc5cf-4d8e-4f8e-a126-9dda81843905",
   "metadata": {
    "editable": false
   },
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b0d95-c290-4a55-8f0c-f879ef6b2a56",
   "metadata": {
    "editable": false
   },
   "source": [
    "# 2b - Statistical Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb4367-5461-4333-928c-a838665bb073",
   "metadata": {
    "editable": false
   },
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f84d9-bd20-41ad-bc47-afc5b7d7b119",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1. Introducing Bias, RMSE, NSE and KGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3706633-d527-4e26-b24b-ca3a5e497878",
   "metadata": {
    "editable": false
   },
   "source": [
    "In tutorial 2a, we have relied on visual inspection to learn about the model performance and to fit of the model output to the observed runoff. For some sets of parameter combinations, it can be difficult to assess which set returns the best result. In this tutorial, we will use evaluation metrics, enabling a more robust comparison between model runs with different parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef81497-9421-49b3-a4d5-5d53d0423b37",
   "metadata": {
    "editable": false
   },
   "source": [
    "**#1 Bias**  \n",
    "Bias is the consistent deviation of simulation results from observed values. It indicates the model's tendency to systematically overestimate or underestimate the target variable.  \n",
    "  \n",
    "Let $y_i$ represent the observed value and $\\hat{y}_i$ denote the simulated value. The bias is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Bias} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where $n$ is the total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14a069-e9fd-4e11-8719-713bdfb6c8ba",
   "metadata": {
    "editable": false
   },
   "source": [
    "**#2 Root Mean Square Error (RMSE)**  \n",
    "RMSE measures the square root of the average squared differences between predicted values and the corresponding actual values (in other words: the square root of the MSE).\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "where $y_i$ represents the observed value, $\\hat{y}_i$ denotes the simulated value, and $n$ is the total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0d7f3-1ecb-49ed-8efb-889e9f3b85c5",
   "metadata": {
    "editable": false
   },
   "source": [
    "**#3 Kling-Gupta Efficiency (KGE)**  \n",
    "KGE is a hydrological metric that assesses the performance of hydrological models by measuring the correlation, bias, and variability of their predictions against observed hydrograph data. It allows evaluation of the model's accuracy, timing, and volume representation.\n",
    "\n",
    "$$\n",
    "\\text{KGE} = 1 - \\sqrt{(r - 1)^2 + (\\alpha - 1)^2 + (\\beta - 1)^2}\n",
    "$$\n",
    "\n",
    "where $r$ represents the Pearson correlation coefficient, $\\alpha$ (alpha) is the ratio of the standard deviations between observed and simulated values, and $\\beta$ (beta) is the ratio of their means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ca7e6-7e69-41e6-9867-9e4278412d56",
   "metadata": {
    "editable": false
   },
   "source": [
    "**#4 Nash-Sutcliffe Efficiency (NSE)**  \n",
    "NSE measures the proportion of the observed variance that is explained by the model results. It is particularly useful for evaluating streamflow predictions. A perfect NSE value of 1 indicates a perfect fit between the model and observed data, while negative values suggest the model performs worse than simply using the mean of the observed values.\n",
    "\n",
    "$$\n",
    "\\text{NSE} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "where $y_i$ represents the observed value, $\\hat{y}_i$ denotes the simulated value, $n$ is the total number of data points, and $\\bar{y}$ is the mean of the observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15540fcc-8bcd-41fd-a94e-30af5b88ea0b",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task I: Understanding the Metrics</h4>\n",
    "    <ol>\n",
    "        <li>Described in your own words what these metrics put their focus on and how they differ.</li>\n",
    "        <li>Based on your answers from the first question: what could be limitations of these metrics and when should they be applied carefully?</li>\n",
    "        <li>If <i>x</i> is the metric value, what does it mean if <i>x<0</i>, <i>x=0</i>, <i>x>0</i>, <i>x=1</i>?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f0f53-cb7e-43a5-9e11-9da0cd639272",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06c208-9e95-4aef-b6ed-9da10cd6891c",
   "metadata": {
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f0f7e-9181-4451-8b6c-fd5c80225673",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 2. Using Bias, RMSE, NSE and KGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f755f-e94e-4189-affe-e54114150c7d",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b214a76-ad32-4776-b819-4db5533d0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "import HBV\n",
    "from ipywidgets import interact, interactive_output, Dropdown, FloatSlider, VBox, Tab, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49142c51-d6c5-4cfa-bc2c-58bb0d5d195e",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Defining Bias, RMSE, NSE and KGE**\n",
    "\n",
    "By the way, the red string at the start of the function which uses three  <code style=\"color:darkred\">\"\"\"</code> is called [docstring](https://realpython.com/documenting-python-code/#documenting-your-python-code-base-using-docstrings). It acts as a description of the function and is also used to describe the arguments and return value. We would suggest that you write docstring whenever the function gets more complicated or its arguments aren't immediately clear.\n",
    "\n",
    "In Jupyter Notebook (or Lab) the documentation can be accessed by pressing `Shift` + `Tab` (both Windows and Mac) when the cursor is placed in the function call. This also works for all other functions and modules, e.g. _numpy_, _scipy_, _matplotlib_, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31254d83-2fa2-4add-9a28-418c8fb422d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(obs, sim):\n",
    "    \"\"\"\n",
    "    Calculate the Bias between observed and simulated values.\n",
    "    \n",
    "    Bias measures the consistent deviation of simulation results from observed values,\n",
    "    indicating whether the model systematically overestimates or underestimates the target variable.\n",
    "    \"\"\"\n",
    "    return np.mean(np.subtract(obs, sim))  # Mean of observation values minus simulation results\n",
    "\n",
    "def abs_bias(obs, sim):\n",
    "    \"\"\"Calculate the absolute bias\"\"\"\n",
    "    return np.abs(bias(obs, sim))\n",
    "\n",
    "def rmse(obs, sim):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square Error (RMSE) between observed and simulated values.\n",
    "    \n",
    "    RMSE measures the square root of the average squared differences between predicted and actual values.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(obs, sim))))\n",
    "\n",
    "def nse(obs, sim):\n",
    "    \"\"\"\n",
    "    Calculate the Nash-Sutcliffe Efficiency (NSE) between observed and simulated values.\n",
    "    \n",
    "    NSE measures the proportion of the observed variance explained by the model results.\n",
    "    A perfect NSE of 1 indicates a perfect fit, while negative values suggest worse performance than using the mean of observed values.\n",
    "    \"\"\"\n",
    "    r_nse = np.corrcoef(obs, sim)[0][1] \n",
    "    alpha_nse = np.divide(np.std(sim), np.std(obs))\n",
    "    beta_nse = np.divide(np.subtract(np.mean(sim), np.mean(obs)), np.std(obs))\n",
    "    nse = 2 * alpha_nse * r_nse - np.square(alpha_nse) - np.square(beta_nse)\n",
    "    return nse\n",
    "\n",
    "def kge(obs, sim):\n",
    "    \"\"\"\n",
    "    Calculate the Kling-Gupta Efficiency (KGE) between observed and simulated values.\n",
    "    \n",
    "    KGE assesses model performance by measuring correlation, bias, and variability against observed data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (correlation coefficient, variation ratio, bias ratio, KGE value)\n",
    "    \"\"\"\n",
    "    r_kge = np.corrcoef(obs, sim)[0][1]  # Pearson correlation coefficient\n",
    "    alpha_kge = np.divide(np.std(sim), np.std(obs))  # Variation ratio\n",
    "    beta_kge = np.divide(np.mean(sim), np.mean(obs))  # Bias\n",
    "    kge = 1 - np.sqrt(np.square(r_kge - 1) + np.square(beta_kge - 1) + np.square(alpha_kge - 1))\n",
    "    return round(r_kge, 3), round(alpha_kge, 3), round(beta_kge, 3), round(kge, 3)\n",
    "\n",
    "def kge_only(obs, sim):\n",
    "    \"\"\"\n",
    "    Calculate the Kling-Gupta Efficiency (KGE) between observed and simulated values.\n",
    "    \n",
    "    Returns only the KGE value, excluding intermediate metrics.\n",
    "    \"\"\"\n",
    "    _, _, _, kge_value = kge(obs, sim)\n",
    "    return kge_value\n",
    "\n",
    "def hbv(par, precip, temp, evap):\n",
    "    # Run HBV snow routine\n",
    "    p_s, _, _ = HBV.snow_routine(par[:4], temp, precip)\n",
    "    # Run HBV runoff simulation\n",
    "    Case = 1 # for now we assume that the preferred path in the upper zone is runoff (Case = 1), it can be set to percolation (Case = 2)\n",
    "    ini = np.array([0,0,0]) # initial state\n",
    "    runoff_sim, _, _ = HBV.hbv_sim(par[4:], p_s, evap, Case, ini)\n",
    "    return runoff_sim\n",
    "\n",
    "def hbv_and_one_obj_fun(par, precip, temp, evap, runoff_obs, n_days, obj_fun):\n",
    "    runoff_sim = hbv(par, precip, temp, evap)\n",
    "    \n",
    "    errors = obj_fun(runoff_obs[n_days:], runoff_sim[n_days:])\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b57d4-8893-49cc-bf0b-cb8257878bb1",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Create and display dropdown for selecting catchment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d6535d-6918-4af8-a91a-3acba1bda0f1",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "source_hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e21d56bf0e479d8fab0e852211eb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Catchment:', options=('Trout River, BC, Canada', 'Medina River, TX, USA', 'Siletz River,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to select the catchment\n",
    "\n",
    "catchment_names = [\"Trout River, BC, Canada\", \"Medina River, TX, USA\", \"Siletz River, OR, USA\"]\n",
    "dropdown = Dropdown(\n",
    "    options=catchment_names,\n",
    "    value=catchment_names[0],\n",
    "    description='Catchment:',\n",
    "    disabled=False)\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d3738-d190-420a-9431-b117e0a5d4ee",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Read catchment data and prepare input data for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d527fac-e269-4d49-a094-dda5b0e9611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read catchment data\n",
    "catchment_name = dropdown.value\n",
    "# Read catchment data\n",
    "file_dic = {catchment_names[0]: \"hysets_10BE007\", catchment_names[1]: \"camels_08178880\", catchment_names[2]: \"camels_14305500\"}\n",
    "df_obs = pd.read_csv(f\"data/{file_dic[catchment_name]}.csv\")\n",
    "# Make sure the date is interpreted as a datetime object -> makes temporal operations easier\n",
    "df_obs.date = pd.to_datetime(df_obs['date'], format='%Y-%m-%d')\n",
    "# Select time frame\n",
    "start_date = '2002-10-01'\n",
    "end_date = '2003-09-30'\n",
    "\n",
    "# Index frame by date\n",
    "df_obs.set_index('date', inplace=True)\n",
    "# Select time frame\n",
    "df_obs = df_obs[start_date:end_date]\n",
    "# Reformat the date for plotting\n",
    "df_obs[\"date\"] = df_obs.index.map(lambda s: s.strftime('%b-%d-%y'))\n",
    "# Reindex\n",
    "df_obs = df_obs.reset_index(drop=True)\n",
    "# Select snow, precip, PET, streamflow and T\n",
    "df_obs = df_obs[[\"snow_depth_water_equivalent_mean\", \"total_precipitation_sum\",\"potential_evaporation_sum\",\"streamflow\", \"temperature_2m_mean\", \"date\"]]\n",
    "# Rename variables\n",
    "df_obs.columns = [\"Snow [mm/day]\", \"P [mm/day]\", \"PET [mm/day]\", \"Q [mm/day]\", \"T [C]\", \"Date\"]\n",
    "\n",
    "# Prepare the data intput for both models\n",
    "P = df_obs[\"P [mm/day]\"].to_numpy()\n",
    "evap = df_obs[\"PET [mm/day]\"].to_numpy()\n",
    "temp = df_obs[\"T [C]\"].to_numpy()    \n",
    "\n",
    "# load calibrated parameters\n",
    "params_calibrated = pd.read_csv(\"./data/calibrated_parameters - HBV.csv\")\n",
    "params_calibrated = params_calibrated[params_calibrated.catchment_name == catchment_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e390b-d4f8-4026-aed4-8a20b2a15e63",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Using bias and RMSE to evaluate HyMOD results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3862d691-9ade-4f65-8358-7afe0bf2315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hbv_runs(obj_funs=bias, lmbda=0.2, **params):\n",
    "\n",
    "    if callable(obj_funs):\n",
    "       obj_funs = [obj_funs, obj_funs] \n",
    "    \n",
    "    # convert params dict to array holding 26 parameters (two runs)\n",
    "    params = np.array(list(params.values()))\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    \n",
    "    # the parameters are for two model runs that can be compared\n",
    "    for i, obj_fun in enumerate(obj_funs):\n",
    "        \n",
    "        # run HBV model and plot output\n",
    "        Q_sim = hbv(params[i*13:(i+1)*13], P, temp, evap)\n",
    "\n",
    "        # Box-Cox transformation\n",
    "        Q_obs = scipy.special.boxcox1p(df_obs[\"Q [mm/day]\"], lmbda)\n",
    "        Q_sim = scipy.special.boxcox1p(Q_sim, lmbda)\n",
    "        \n",
    "        # evaluate the objective function\n",
    "        obj_fun_value = obj_fun(df_obs[\"Q [mm/day]\"], Q_sim)\n",
    "        plt.plot(df_obs[\"Date\"], Q_sim, color=[\"red\", \"blue\"][i], label=f\"Model Run #{i + 1}\\n{obj_fun.__name__}: {obj_fun_value:.2f}\")\n",
    "\n",
    "    plt.plot(df_obs[\"Date\"], Q_obs, color=\"black\", label=\"Observed\")\n",
    "    \n",
    "    plt.title(catchment_name + f\"\\n(Box-Cox transformed)\")\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(mdate.MonthLocator(bymonth=range(1,13,4)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a7a4bc-7b6c-42c8-bfbc-67e1b18b3657",
   "metadata": {
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7bc9cafb8741d28bf6f8e33cde7b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(FloatSlider(value=0.5, description='Ts', max=3.0, min=-3.0), FloatSlider(value=9.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df7d07231d4ffd8d5935df6842dcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT ALTER! parameter definitions for easy input\n",
    "\n",
    "param_names = [\"Ts\", \"CFMAX\", \"CFR\", \"CWH\", \"BETA\", \"LP\", \"FC\", \"PERC\", \"K0\", \"K1\", \"K2\", \"UZL\", \"MAXBAS\"]\n",
    "lower = [-3, 0, 0, 0, 0, 0.3, 1, 0, 0.05, 0.01, 0.005, 0, 1] # lower bounds for the parameters\n",
    "upper = [3, 20, 1, 0.8, 7, 1, 2000, 100, 2, 1, 0.1, 100, 6]  # upper bounds for the parameters\n",
    "\n",
    "# slider for Box-Cox lambda\n",
    "lmbda = FloatSlider(value=0, min=-2, max=2, description=\"Lambda\")\n",
    "\n",
    "# widgets for easy input\n",
    "params = {f\"{param}_{i}\": FloatSlider(value=np.round(params_calibrated.iloc[i, j+3], 1), min=xmin, max=xmax, step=0.1, description=param) for i in range(2) for j, xmin, xmax, param in zip(range(13), lower, upper, param_names)}\n",
    "tabs   = Tab([VBox(list(params.values())[:13] + [lmbda,]), VBox(list(params.values())[13:] + [lmbda,])])\n",
    "tabs.set_title(0, \"First Run (Red)\")\n",
    "tabs.set_title(1, \"Second Run (Blue)\")\n",
    "\n",
    "display(tabs)\n",
    "interactive_output(compare_hbv_runs, params | {\"lmbda\": lmbda, \"obj_funs\": fixed([bias, rmse])}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e5d45-1d05-4122-8a9b-8f49deb86166",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task II: Apply and Bias and RMSE</h4>    \n",
    "    In the above plot you see two HBV runs for the catchment that you selected. One was calibrated using the evaluation metric \"bias\" (red) and one using the \"rmse\" (blue).\n",
    "    <ol>\n",
    "        <li>Compare the two simulated hydrographs. What differences can you see between the two metrics. Compare this to your answers to Task I.</li>\n",
    "        <li>What features of the hydrograph do the two metrics pick up or miss? Again, think <i>timing, magnitude, ...</i> </li>\n",
    "        <li>Under which conditions would you choose which metric?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cbc5d-8476-4ee6-82a7-8628a5fae926",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05347bc0-7b21-4c3d-90cf-b7e737a3e16d",
   "metadata": {
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f7bdd-4895-4ab9-9442-945be90570ba",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Using NSE and KGE to evaluate HyMOD results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f33702-1cff-41e1-ba12-b36ce27d78f9",
   "metadata": {
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7658a0f2c474a20a92694dcc7a442f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(FloatSlider(value=-2.9, description='Ts', max=3.0, min=-3.0), FloatSlider(value=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19aaca61df44fcb8a1043ee9f23b95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT ALTER! Code to compare NSE and KGE\n",
    "\n",
    "# slider for Box-Cox lambda\n",
    "lmbda = FloatSlider(value=0, min=-2, max=2, description=\"Lambda\")\n",
    "\n",
    "# widgets for easy input\n",
    "params = {f\"{param}_{i}\": FloatSlider(value=np.round(params_calibrated.iloc[2, j+3], 1), min=xmin, max=xmax, step=0.1, description=param) for i in range(2) for j, xmin, xmax, param in zip(range(13), lower, upper, param_names)}\n",
    "tabs   = Tab([VBox(list(params.values())[:13] + [lmbda,]), VBox(list(params.values())[13:] + [lmbda,])])\n",
    "tabs.set_title(0, \"First Run (Red)\")\n",
    "tabs.set_title(1, \"Second Run (Blue)\")\n",
    "\n",
    "display(tabs)\n",
    "interactive_output(compare_hbv_runs, params | {\"lmbda\": lmbda, \"obj_funs\": fixed([nse, kge_only])}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339afca0-dee9-4434-9364-a11f8db89ad1",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task III: Apply and Discuss NSE and KGE</h4>\n",
    "    In the above plots you again find two model runs. They initially both use the same set of parameters. Only the evaluation metric displayed in the legend is different (\"nse\" for the red run and \"kge\" for the blue run). As you can see, the same set of parameters (e.g. same model run) can lead to different values of the two evaluation metrics.\n",
    "    <ol>\n",
    "        <li>Compare the values of NSE and KGE by indiviudally tuning the paramters of both runs. How can you increase the values closer to their optimal of one?</li>\n",
    "        <li>Are the two metrics affected differently by individual parameters?</li>\n",
    "        <li>Which component of KGE dominates the result? (reminder: in KGE, r represents the correlation coefficient, alpha is the ratio of the standard deviations between observed and simulated values, and beta is the ratio of their means)</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c9687-800b-4762-be3b-696980187a13",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a5a35-52bc-4f8b-abaa-3a680040addf",
   "metadata": {
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416c4d0-d766-4261-a73f-6dc17555f0ce",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Relationships between the Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8812ba96-cf76-4a36-b5f5-eb0d55250673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e194d8c97f744421b6c7bed2e52a15a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='lmbda', max=2.0, min=-2.0), Output()), _dom_classes=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "@interact(lmbda=(-2, 2, 0.1))\n",
    "def multiple_objectives(lmbda):\n",
    "    \n",
    "    # the function needs to be defined here to use the lambda parameter\n",
    "    def bc_bias(obs, sim):\n",
    "        return abs_bias(scipy.special.boxcox1p(obs, lmbda), scipy.special.boxcox1p(sim, lmbda))\n",
    "\n",
    "    def plot_multiple_objectives(obj_funs=(bias, rmse, nse, kge_only)):\n",
    "\n",
    "        n = len(obj_funs)\n",
    "        \n",
    "        # generate n_samples parameter sets from a uniform distribution\n",
    "        n_samples = 200\n",
    "        X0 = np.random.uniform(low=lower, high=upper, size=(n_samples, 13))\n",
    "    \n",
    "        # evaluate the different objective functions for all samples\n",
    "        obj_fun_evaluations = dict()\n",
    "        for obj_fun in obj_funs:\n",
    "            obj_fun_evaluations[obj_fun] = [hbv_and_one_obj_fun(x0, P, temp, evap, df_obs[\"Q [mm/day]\"].to_numpy(), 0, obj_fun) for x0 in X0]\n",
    "    \n",
    "        # scatter plots of all combinations of objective functions\n",
    "        obj_fun_combinations = list(itertools.combinations(obj_funs, 2))\n",
    "    \n",
    "        fig, axs = plt.subplots(n - 1, n - 1, figsize=(4*(n - 1), 4*(n - 1)))\n",
    "\n",
    "        for ax in axs.ravel():\n",
    "            ax.axis(\"off\")\n",
    "        \n",
    "        for i, j in itertools.combinations(range(n), 2):\n",
    "            fun1 = obj_funs[j]\n",
    "            fun2 = obj_funs[i]\n",
    "            ax = axs[i, 3 - j]\n",
    "            ax.set_xlabel(fun1.__name__)\n",
    "            ax.set_ylabel(fun2.__name__)\n",
    "            ax.scatter(obj_fun_evaluations[fun1], obj_fun_evaluations[fun2], alpha=0.5)\n",
    "            ax.grid()\n",
    "            ax.axis(\"on\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_multiple_objectives()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67924c-7cff-4aa7-ab5d-3bd52d28880d",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task IV: Relationships Between Evaluation Metrics</h4>\n",
    "    For the above plots we have run HBV 200 times using random parameter sets within the usually used ranges. For each model run, we calculated the bias, RMSE and NSE and plotted them in multi-objective scatterplots. This allows you to compare the general relationships between the different metrics.\n",
    "    <ol>\n",
    "        <li>Building on the answers you gave earlier, continue your discussion on how bias, RMSE and NSE are related.</li>\n",
    "        <li>Can you define regions in which the metrics behave similarly?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915a44f-303c-4a86-87a1-1fe56d127524",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c74648-5905-4294-b7a2-1a193bcd5c78",
   "metadata": {
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

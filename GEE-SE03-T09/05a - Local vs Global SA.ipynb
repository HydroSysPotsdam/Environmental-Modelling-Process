{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510796dc-ca50-4522-b9b4-a08594a69d84",
   "metadata": {
    "editable": false
   },
   "source": [
    "![](./figures/Logo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349b8fc-0238-425e-b304-9c290466432f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Please click the <span>&#x23E9;</span> button to run all cells before you start working on the notebook ...\n",
    "\n",
    "## In this part of the tutorial, you will\n",
    "* compare local and global sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b750e-3c21-4fec-8226-15c59197c03d",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22382e4-4131-4156-b440-e5410a3420cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 5 a - Local and Regional Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da79fd7-35cc-4af6-aa56-7dca452c3b13",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f66fb-496e-49a8-a9e6-190a0125d3f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1 Loading Catchment Dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728278b7-5f78-49b6-8679-38c1463c19dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import HBV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import RSA_thres as RSA           # module to perform RSA with threshold\n",
    "import plot_functions\n",
    "import sampling\n",
    "from ipywidgets import interact, Dropdown, FloatSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a55e8e-c1d9-4d7b-92b0-2df1a93f18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, sim, spinup=365):\n",
    "    obs = obs[spinup:]\n",
    "    sim = sim[spinup:]\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(obs, sim))))\n",
    "\n",
    "def abs_bias(obs, sim, spinup=365):\n",
    "    obs = obs[spinup:]\n",
    "    sim = sim[spinup:]\n",
    "    return np.mean(np.abs(sim - obs))\n",
    "\n",
    "def hbv(par, precip, temp, evap):\n",
    "    # Run HBV snow routine\n",
    "    p_s, _, _ = HBV.snow_routine(par[:4], temp, precip)\n",
    "    # Run HBV runoff simulation\n",
    "    Case = 1 # for now we assume that the preferred path in the upper zone is runoff (Case = 1), it can be set to percolation (Case = 2)\n",
    "    ini = np.array([0,0,0]) # initial state\n",
    "    runoff_sim, _, _ = HBV.hbv_sim(par[4:], p_s, evap, Case, ini)\n",
    "    return runoff_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc102bf-b465-49d4-ab12-715784e16e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd434a49cb7f42c7935cc41499bd3297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Catchment:', options=('Medina River, TX, USA', 'Siletz River, OR, USA', 'Trout River, BC…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to select the catchment\n",
    "\n",
    "catchment_names = [\"Medina River, TX, USA\", \"Siletz River, OR, USA\", \"Trout River, BC, Canada\"]\n",
    "dropdown = Dropdown(\n",
    "    options=catchment_names,\n",
    "    value=catchment_names[0],\n",
    "    description='Catchment:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d420feb-7e9d-404f-b1b8-a4dc150505f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read catchment data\n",
    "catchment_name = dropdown.value\n",
    "file_dic = {catchment_names[0]: \"camels_08178880\", catchment_names[1]: \"camels_14305500\", catchment_names[2]: \"hysets_10BE007\"}\n",
    "df_obs = pd.read_csv(f\"data/{file_dic[catchment_name]}.csv\")\n",
    "\n",
    "# correctly load the date and restrict analysis to one year\n",
    "df_obs.date = pd.to_datetime(df_obs['date'], format='%Y-%m-%d')\n",
    "start_date  = '2003-01-01' # the first year is used as spinup\n",
    "end_date    = '2008-12-30'\n",
    "\n",
    "# Index frame by date\n",
    "df_obs.set_index('date', inplace=True)\n",
    "# Select time frame\n",
    "df_obs = df_obs[start_date:end_date]\n",
    "# Reformat the date for plotting\n",
    "df_obs[\"date\"] = df_obs.index.map(lambda s: s.strftime('%b-%d-%y'))\n",
    "# Reindex\n",
    "df_obs = df_obs.reset_index(drop=True)\n",
    "# Select snow, precip, PET, streamflow and T\n",
    "df_obs = df_obs[[\"snow_depth_water_equivalent_mean\", \"total_precipitation_sum\",\"potential_evaporation_sum\",\"streamflow\", \"temperature_2m_mean\", \"date\"]]\n",
    "# Rename variables\n",
    "df_obs.columns = [\"Snow [mm/day]\", \"P [mm/day]\", \"PET [mm/day]\", \"Q [mm/day]\", \"T [C]\", \"Date\"]\n",
    "\n",
    "# load calibrated parameters\n",
    "params_calibrated = pd.read_csv(\"./data/calibrated_parameters - HBV.csv\")\n",
    "params_calibrated = params_calibrated[(params_calibrated.catchment_name == catchment_name) & (params_calibrated.objective_function == \"nse\")] # use only this catchment and the rmse parameters\n",
    "params_calibrated = params_calibrated.iloc[0,3:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ecfcb1-d303-4915-8ece-48ac96a5e068",
   "metadata": {},
   "source": [
    "## 2 Global Sensitivity Analysis with AAT Sampling and Difference of CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825eb736-6729-4d2c-84da-fd9b43ce0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 250\n",
    "\n",
    "param_names = [\"Ts\", \"CFMAX\", \"CFR\", \"CWH\", \"BETA\", \"LP\", \"FC\", \"PERC\", \"K0\", \"K1\", \"K2\", \"UZL\", \"MAXBAS\"]\n",
    "lower       = np.array([-3, 0, 0, 0, 0, 0.3, 1, 0, 0.05, 0.01, 0.005, 0, 1])\n",
    "upper       = np.array([3, 20, 1, 0.8, 7, 1, 2000, 100, 2, 1, 0.1, 100, 6])\n",
    "\n",
    "# take N samples using a latin-hypercube strategy from the whole paramters space\n",
    "np.random.seed(1234)\n",
    "samples = sampling.AAT_sampling(\"lhs\", len(param_names), sp.stats.uniform, np.array([lower, upper - lower]).T.tolist(), N=N)\n",
    "\n",
    "# evaluate HBV for these parameters\n",
    "errors = np.zeros((N, 2))\n",
    "for i in range(N):\n",
    "    Q_sim = hbv(samples[i], df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])\n",
    "    errors[i, 0] = abs_bias(df_obs[\"Q [mm/day]\"], Q_sim) # calculate bias for this paramter set\n",
    "    errors[i, 1] =     rmse(df_obs[\"Q [mm/day]\"], Q_sim) # calculate rmse for this paramter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb93078-b542-4d3a-9672-a7f433de613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d608c09f8cef4cb2b1e6fcccf3dec837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3698953205832412, description='|Bias|', max=1.603661667104514, min=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(bias=FloatSlider(value=np.quantile(errors[:,0], 0.2), min=errors[:,0].min(), max=errors[:,0].max(), step=0.005, description=\"|Bias|\"), rmse=FloatSlider(value=np.quantile(errors[:,1], 0.2), min=errors[:,1].min(), max=errors[:,1].max(), step=0.005, description=\"RMSE\"))\n",
    "def plot_behavioral(bias, rmse):\n",
    "    \n",
    "    # grid layout for plot\n",
    "    fig = plt.figure(figsize=(4*3, 3*3))\n",
    "    gsl = fig.add_gridspec(3, 4)\n",
    "\n",
    "    # subplot for the sensitivity values\n",
    "    ax_sens = fig.add_subplot(gsl[0,:])\n",
    "    ax_sens.set_xticks(range(13), param_names)\n",
    "    ax_sens.set_title(\"Paramter Importance Based on Max. Diff. in CDF\")\n",
    "    ax_sens.set_ylabel(\"Max. Diff. in CDF\")\n",
    "    \n",
    "    for i, metric in enumerate([\"|Bias|\", \"RMSE\"]):\n",
    "\n",
    "        # which paramters are behavioral\n",
    "        behavioral = (errors[:,i] <= [bias, rmse][i])\n",
    "\n",
    "        # calculate experimental CDFs for the behavioral and non-behavioral parameters\n",
    "        ecdfs = np.array([[sp.stats.ecdf(samples[mask, k]).cdf for k in range(13)] for mask in [behavioral, ~behavioral]])\n",
    "        \n",
    "        for j, param in enumerate([\"CFMAX\", \"BETA\", \"FC\", \"K2\"]):\n",
    "    \n",
    "            # index of the parameter value\n",
    "            k = param_names.index(param)\n",
    "            x = np.linspace(lower[k], upper[k], 100)\n",
    "            \n",
    "            ax = fig.add_subplot(gsl[i + 1, j])\n",
    "            if j == 0: ax.set_title(f\"Behavioral using {metric}\")\n",
    "            if j == 0: ax.set_ylabel(\"CDF\")\n",
    "            if j != 0: ax.set_yticklabels([])\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_xlim(samples[:,k].min(), samples[:,k].max())\n",
    "            ax.set_ylim([-0.05, 1.05])\n",
    "    \n",
    "            # plot cdf of behavioral\n",
    "            if behavioral.sum() > 1:\n",
    "                ax.step(x, [ecdfs[0, k].evaluate(xi) for xi in x], where=\"mid\", color=\"blue\", label=f\"{np.mean(behavioral):2.0%} Behavioral\")\n",
    "    \n",
    "            # plot cdf of nonbehavioral\n",
    "            if behavioral.sum() < len(behavioral):\n",
    "                ax.step(x, [ecdfs[1, k].evaluate(xi) for xi in x], where=\"mid\", color=\"red\", label=f\"{1 - np.mean(behavioral):2.0%} Nonbehavioral\")\n",
    "    \n",
    "            # plot maximum difference when both curves are given\n",
    "            if 1 < behavioral.sum() < len(behavioral):\n",
    "                argmax = np.argmax(np.abs([ecdfs[0, k].evaluate(xi) - ecdfs[1, k].evaluate(xi) for xi in x]))\n",
    "                rect = ax.add_patch(plt.matplotlib.patches.Rectangle((x[argmax - 1], ecdfs[0, k].evaluate(x[argmax])), x[argmax + 1] - x[argmax - 1], ecdfs[1, k].evaluate(x[argmax]) - ecdfs[0, k].evaluate(x[argmax])))\n",
    "                rect.set(color=f\"C{k}\", hatch=None if i==0 else \"//\", edgecolor=\"white\")\n",
    "            \n",
    "            # rugplot of the behavioral paramters\n",
    "            ax.plot(samples[ behavioral,k], -0.025 + np.zeros_like(samples[ behavioral,k]), '|', color='blue', alpha=0.2) \n",
    "            ax.plot(samples[~behavioral,k],  1.025 + np.zeros_like(samples[~behavioral,k]), '|', color='red',  alpha=0.2)\n",
    "\n",
    "            if j == 3: ax.legend()\n",
    "    \n",
    "        for j, param in enumerate(param_names):\n",
    "    \n",
    "            # index of the paramter value\n",
    "            k = param_names.index(param)\n",
    "            x = np.linspace(lower[k], upper[k], 100)\n",
    "    \n",
    "            if 1 < behavioral.sum() < len(behavioral):\n",
    "                diffmax = np.max(np.abs([ecdfs[0, k].evaluate(xi) - ecdfs[1, k].evaluate(xi) for xi in x]))\n",
    "                ax_sens.bar(j - 0.17 if i == 0 else j + 0.17, diffmax, width=0.3, color=f\"C{k}\", hatch=None if i == 0 else \"//\", edgecolor=\"white\", label=None if param != \"BETA\" else [\"|Bias|\", \"RMSE\"][i])\n",
    "\n",
    "\n",
    "    ax_sens.legend()    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ed6b7-6851-4e8a-9c2f-f9a8c20c5cd9",
   "metadata": {
    "editable": true
   },
   "source": [
    "<div style=\"background:#e0f2fe;border:1mm solid SkyBlue; padding:1%\">\n",
    "    <h4><span>&#129300 </span>Task I: Global Sensitivity Analysis</h4>\n",
    "    <p>\n",
    "        Similar to what we did in the last tutorial, we have again ran HBV for parameter combinations samples from the whole parameter space using a LHS strategy. These runs are again classified as behavioral when the RMSE or |Bias| are below a certain threshold, which you can set. In constrast to last weeks tutorial, the runs do not need to fulfill both conditions, meaning that we find different behavioral parameter sets for the two metrics.\n",
    "    </p>\n",
    "    <p>\n",
    "        The above plot shows the empirical cumulative density function (CDF) of the behavioral and nonbehavioral parameters for the two metrics. Based on the maximum absolute difference between the CDFs for behavioral and nonbehavioral curves we can define a measure of parameter importance. In other words: if a paramter is important for the evaluation of the model run, it should influence which parameter sets are classificed as behavioral. The first row in the plot shows this measure for each parameter and metric.\n",
    "    </p>\n",
    "    <ol>\n",
    "        <li>What are benefits and shortcomings to define \"importance\" using this approach?</li>\n",
    "        <li>Which parameters are important? Are there differences between metrics?</li>\n",
    "        <li>How do the thresholds influence parameter importance? How would you choose a limit for screening important paramters?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999216c-fbc0-4a73-a153-9987a6dec15f",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d496cc3-8740-4e87-8374-743334b24fa8",
   "metadata": {},
   "source": [
    "<div style=\"background:#e0f2fe;border:1mm solid SkyBlue; padding:1%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec412330-2ad8-49da-a470-3ba16bfa18d2",
   "metadata": {},
   "source": [
    "## 3 Local Sensitivity Analysis with OAT and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d5438a-2062-4e9c-b26f-ef9d8f977ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eafca6717d247798b9e1805b5111c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='$\\\\Delta x$', max=0.2, min=0.01, readout_format='.0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(r=FloatSlider(value=0.1, min=0.01, max=0.2, step=0.01, readout_format=\".0%\", description=r\"$\\Delta x$\"))\n",
    "def plot_OAT(r=0.1):\n",
    "    \n",
    "    # grid layout for plot\n",
    "    fig = plt.figure(figsize=(4*3, 3*3))\n",
    "    gsl = fig.add_gridspec(3, 4)\n",
    "\n",
    "    # subplot for the sensitivity values\n",
    "    ax_sens = fig.add_subplot(gsl[0,:])\n",
    "    ax_sens.set_xticks(range(13), param_names)\n",
    "    ax_sens.set_title(\"Paramter Importance Based on Local Relative Derivative\")\n",
    "    ax_sens.set_ylabel(\"Local Relative Derivative\")\n",
    "    \n",
    "    for i, metric in enumerate([\"|Bias|\", \"RMSE\"]):\n",
    "        \n",
    "        for k, param in enumerate(param_names):\n",
    "\n",
    "            # calibrated parameter set\n",
    "            csample = params_calibrated.copy()\n",
    "            Q_sim   = hbv(csample, df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])\n",
    "            csample = csample\n",
    "            cmetric = [abs_bias, rmse][i](df_obs[\"Q [mm/day]\"], Q_sim)\n",
    "            cmetric = cmetric\n",
    "\n",
    "            # percentage change to lower values\n",
    "            lsample    = params_calibrated.copy()\n",
    "            lsample[k] = lsample[k]*(1 - r)\n",
    "            Q_sim      = hbv(lsample, df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])\n",
    "            lsample    = lsample/csample\n",
    "            lmetric    = [abs_bias, rmse][i](df_obs[\"Q [mm/day]\"], Q_sim)\n",
    "            lmetric    = lmetric/cmetric\n",
    "\n",
    "            # percentage change to higher values\n",
    "            rsample    = params_calibrated.copy()\n",
    "            rsample[k] = rsample[k]*(1 + r)\n",
    "            Q_sim      = hbv(rsample, df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])\n",
    "            rsample    = rsample/csample\n",
    "            rmetric    = [abs_bias, rmse][i](df_obs[\"Q [mm/day]\"], Q_sim)\n",
    "            rmetric    = rmetric/cmetric\n",
    "\n",
    "            csample = csample/csample\n",
    "            cmetric = cmetric/cmetric\n",
    "            \n",
    "            if param in [\"CFMAX\", \"BETA\", \"FC\", \"K2\"]:\n",
    "                \n",
    "                # index of the parameter value\n",
    "                j = [\"CFMAX\", \"BETA\", \"FC\", \"K2\"].index(param)\n",
    "                \n",
    "                ax = fig.add_subplot(gsl[i + 1, j])\n",
    "                if j == 0: ax.set_ylabel(f\"Deviation of {metric}\")\n",
    "                if j != 0: ax.set_yticks([0.8, 1, 1.2], [])\n",
    "                if j == 0: ax.set_yticks([0.8, 1, 1.2], [\"-20%\", \"0%\", \"+20%\"])\n",
    "                ax.set_ylim([0.8, 1.2])\n",
    "                ax.set_xlabel(f\"Deviation from {param}\")\n",
    "                ax.set_xlim(0.8, 1.2)\n",
    "                ax.set_xticks([0.8, 1.0, 1.2], [\"-20%\", \"0%\", \"+20%\"])\n",
    "                \n",
    "                #ax.axvline(csample[k], color=\"gray\")\n",
    "                ax.scatter(csample[k], cmetric, color=\"gray\", label=\"Calibrated\")\n",
    "                ax.scatter(lsample[k], lmetric, c=f\"C{k}\", label=\"OAT\")\n",
    "                ax.scatter(rsample[k], rmetric, c=f\"C{k}\")\n",
    "                ax.plot([lsample[k], csample[k], rsample[k]], [lmetric, cmetric, rmetric], color=f\"C{k}\", zorder=0)\n",
    "                \n",
    "                if j == 3: ax.legend()\n",
    "\n",
    "            meanderiv = 0.5*abs(rmetric - cmetric)/(rsample[k] - csample[k]) + 0.5*abs(cmetric - lmetric)/(csample[k] - lsample[k])\n",
    "            ax_sens.bar(k - 0.17 if i == 0 else k + 0.17, meanderiv, width=0.3, color=f\"C{k}\", hatch=None if i == 0 else \"//\", edgecolor=\"white\", label=None if param != \"BETA\" else [\"|Bias|\", \"RMSE\"][i])\n",
    "            \n",
    "    \n",
    "\n",
    "    ax_sens.legend()    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8445413-b9bb-45ff-ac62-6083c5b9ecb2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<div style=\"background:#e0f2fe;border:1mm solid SkyBlue; padding:1%\">\n",
    "    <h4><span>&#129300 </span>Task II: Local Sensitivity Analysis</h4>\n",
    "    <p>\n",
    "        We now take a step back and look at a more local approach. Around the calibrated model (e.g. the paramter set that has lowest metrics), we explore the paramter space using one-at-a-time sampling. This means that we only alter one parameter at a time, with a relative step size that you are free to set. We do this in both directions, so that for each parameter, we have three metric values: the step to lower values, the calibrated value and the step to higher values. They are drawn in the lower two rows of the plot, again once for RMSE and once for |Bias|.\n",
    "    </p>\n",
    "    <p>\n",
    "        We can now use the average partial derivative around the calibrated value as a measure for the importance of the parameter. In other words: If a paramter is important, its choice should change the metric and therefore result in a higher derivative. In the first row of the plot, we calculated this derivative for both parameters and metrics.\n",
    "    </p>\n",
    "    <ol>\n",
    "        <li>What are benefits and shortcomings to define \"importance\" using this approach?</li>\n",
    "        <li>Which parameters are important? Are there differences between metrics?</li>\n",
    "        <li>How does the step size influence parameter importance? How would you choose a limit for screening important paramters?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e338b-e2c3-4cc1-9abd-ee182eb8ff55",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a980553-23b4-412a-ae88-7e5d3e585848",
   "metadata": {},
   "source": [
    "<div style=\"background:#e0f2fe;border:1mm solid SkyBlue; padding:1%\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8b84d3-eeb9-4f77-ae70-0931d395fbb7",
   "metadata": {
    "editable": false
   },
   "source": [
    "![](./figures/Logo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a28818-1e44-4f22-93e0-188e28135eda",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Please click the <span>&#x23E9;</span> button to run all cells before you start working on the notebook ...\n",
    "\n",
    "## In this part of the tutorial, you will\n",
    "* learn about different sampling strategies\n",
    "* compare how these strategies result in different model evaluations \n",
    "* think of and implement your own method of accessing which paramter is \"relevant\" in a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd8049-bfb8-45db-8f25-9663c39aa3e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a30a52-69be-4849-8c97-5b493860f365",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3 Sampling of Input Paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f2ce3-c61d-42b9-a3bb-c38bbaacd33f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a41445-a335-4f30-8878-07bf16c69b41",
   "metadata": {
    "editable": true
   },
   "source": [
    "Sampling involves selecting a subset of potential input parameters from defined ranges. A goal of the process is to understand how variations in input parameters impact the output of computer models, aiding in model optimization and analysis. Typically, three different sampling strategies are used:\n",
    "\n",
    "\n",
    "#### Grid Sampling\n",
    "Input parameters are selected at even intervals within specified parameter bounds. A derived approach is **stratified sampling**, where the parameters are not assigned to the grid intersections but are random point is drawn in each grid cell. \n",
    "\n",
    "#### Random Sampling\n",
    "Random sampling within parameter bounds from a uniform distribution. This is also known as Monte-Carlo sampling.\n",
    "\n",
    "#### Latin Hypercube Sampling (LHS)\n",
    "Systematic and efficient technique for selecting a diverse set of input parameter combinations within defined ranges, minimizing redundancy and ensuring a more representative coverage of the parameter space in computational experiments.\n",
    "\n",
    "It work's like this:\n",
    "1. Parameter Ranges Definition: Define the ranges for each input parameter that you want to sample in your study.\n",
    "2. Divide Ranges: Divide each parameter range into equal intervals, corresponding to the desired number of samples or scenarios.\n",
    "3. Create a Matrix: Create a Latin square matrix, where each row and column represents one interval of each parameter. The Latin property ensures that each interval is sampled exactly once across rows and columns.\n",
    "4. Random Permutation: Randomly permute the elements within each row of the matrix, ensuring that the samples are selected randomly within their respective intervals.\n",
    "5. Select Samples: Choose one sample from each row of the matrix. These samples represent the selected combinations of input parameters for your simulation or experiment.\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/publication/347334888/figure/fig1/AS:976080278138880@1609727081034/Comparison-of-random-stratified-and-latin-hypercube-samplings-with-16-points-d-2-M.png\" style=\"max-width:50%\"/></center>\n",
    "\n",
    "| **Method**          | **Coverage**                       | **Bias**                 | **Complexity**  | **Marginal Distribution** |\n",
    "|---------------------|------------------------------------|--------------------------|-----------------|---------------------------|\n",
    "| Grid                | evenly spaced                      | low                      | simple          | discrete |\n",
    "| Stratified          | randomly distributed               | low                      | simple          | roughly uniform |\n",
    "| Random              | randomly distributed               | moderate (can be skewed) | simple          | roughly uniform  |\n",
    "| LHS                 | evenly distributed in intervals    | low                      | moderate        | uniform  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb3542-8e24-4bbc-ae8a-826c5fb1b2c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Loading Catchment Dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74cf63d-751a-4c3e-b1b4-1cb3ee783005",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import HBV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numbers import Number\n",
    "from scipy.stats.qmc import LatinHypercube, scale\n",
    "from ipywidgets import interact, interactive_output, Dropdown, Checkbox, VBox, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fc9cd6-f073-4dde-86b9-60baebfa63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, sim, spinup=365):\n",
    "    obs = obs[spinup:]\n",
    "    sim = sim[spinup:]\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(obs, sim))))\n",
    "\n",
    "def nse(obs, sim, spinup=365):\n",
    "    obs = obs[spinup:]\n",
    "    sim = sim[spinup:]\n",
    "    r_nse     = np.corrcoef(obs, sim)[0][1] \n",
    "    alpha_nse = np.divide(np.std(sim), np.std(obs))\n",
    "    beta_nse  = np.divide(np.subtract(np.mean(sim), np.mean(obs)), np.std(obs))\n",
    "    return 2 * alpha_nse * r_nse - np.square(alpha_nse) - np.square(beta_nse)\n",
    "\n",
    "def hbv(par, precip, temp, evap):\n",
    "    # Run HBV snow routine\n",
    "    p_s, _, _ = HBV.snow_routine(par[:4], temp, precip)\n",
    "    # Run HBV runoff simulation\n",
    "    Case = 1 # for now we assume that the preferred path in the upper zone is runoff (Case = 1), it can be set to percolation (Case = 2)\n",
    "    ini = np.array([0,0,0]) # initial state\n",
    "    runoff_sim, _, _ = HBV.hbv_sim(par[4:], p_s, evap, Case, ini)\n",
    "    return runoff_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013c86ff-fec5-40ad-8fdd-ea59c187f31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2595240e9f8492fabf161268c8b5aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Catchment:', options=('Medina River, TX, USA', 'Siletz River, OR, USA', 'Trout River, BC…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to select the catchment\n",
    "\n",
    "catchment_names = [\"Medina River, TX, USA\", \"Siletz River, OR, USA\", \"Trout River, BC, Canada\"]\n",
    "dropdown = Dropdown(\n",
    "    options=catchment_names,\n",
    "    value=catchment_names[0],\n",
    "    description='Catchment:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2056535f-60cf-452d-af1a-a4f088a19438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read catchment data\n",
    "catchment_name = dropdown.value\n",
    "file_dic = {catchment_names[0]: \"camels_08178880\", catchment_names[1]: \"camels_14305500\", catchment_names[2]: \"hysets_10BE007\"}\n",
    "df_obs = pd.read_csv(f\"data/{file_dic[catchment_name]}.csv\")\n",
    "\n",
    "# correctly load the date and restrict analysis to one year\n",
    "df_obs.date = pd.to_datetime(df_obs['date'], format='%Y-%m-%d')\n",
    "start_date  = '2003-01-01' # the first year is used as spinup\n",
    "end_date    = '2005-12-30'\n",
    "\n",
    "# Index frame by date\n",
    "df_obs.set_index('date', inplace=True)\n",
    "# Select time frame\n",
    "df_obs = df_obs[start_date:end_date]\n",
    "# Reformat the date for plotting\n",
    "df_obs[\"date\"] = df_obs.index.map(lambda s: s.strftime('%b-%d-%y'))\n",
    "# Reindex\n",
    "df_obs = df_obs.reset_index(drop=True)\n",
    "# Select snow, precip, PET, streamflow and T\n",
    "df_obs = df_obs[[\"snow_depth_water_equivalent_mean\", \"total_precipitation_sum\",\"potential_evaporation_sum\",\"streamflow\", \"temperature_2m_mean\", \"date\"]]\n",
    "# Rename variables\n",
    "df_obs.columns = [\"Snow [mm/day]\", \"P [mm/day]\", \"PET [mm/day]\", \"Q [mm/day]\", \"T [C]\", \"Date\"]\n",
    "\n",
    "# load calibrated parameters\n",
    "params_calibrated = pd.read_csv(\"./data/calibrated_parameters - HBV.csv\")\n",
    "params_calibrated = params_calibrated[(params_calibrated.catchment_name == catchment_name) & (params_calibrated.objective_function == \"nse\")] # use only this catchment and the rmse parameters\n",
    "params_calibrated = params_calibrated.iloc[0,3:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122cbac-8daa-4da1-8d4f-3937e123c66a",
   "metadata": {},
   "source": [
    "## Comparing Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7b010b-8bed-4f65-a6f3-7b3ad025fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [\"Ts\", \"CFMAX\", \"CFR\", \"CWH\", \"BETA\", \"LP\", \"FC\", \"PERC\", \"K0\", \"K1\", \"K2\", \"UZL\", \"MAXBAS\"]\n",
    "lower       = np.array([-3, 0, 0, 0, 0, 0.3, 1, 0, 0.05, 0.01, 0.005, 0, 1])\n",
    "upper       = np.array([3, 20, 1, 0.8, 7, 1, 2000, 100, 2, 1, 0.1, 100, 6])\n",
    "\n",
    "# restrict the paramter range around a calibrated solution\n",
    "ranges = list(zip(params_calibrated, lower, upper))\n",
    "lower  = np.array([max(low, value - 0.1*(high - low)) for value, low, high in ranges])\n",
    "upper  = np.array([min(value + 0.1*(high - low), high) for value, low, high in ranges])\n",
    "\n",
    "def uniform(low, high, n=100, **kwargs):\n",
    "    p = len(low)\n",
    "    return np.random.uniform(low, high, (n, p))\n",
    "\n",
    "def grid(low, high, n=100, m=5, **kwargs):\n",
    "    dims = [np.linspace(l, h, m) for l, h in zip(low, high)]\n",
    "    return np.array([np.random.choice(dim, n) for dim in dims]).T\n",
    "\n",
    "def lhs(low, high, n=100, **kwargs):\n",
    "    return scale(LatinHypercube(len(low)).random(n), low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0028fd3-26f1-48d1-8de1-2ed3f70adb13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab1b16a0f054975a59bef6ea75b39c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='First P.', options=(('Ts', 0), ('CFMAX', 1), ('CFR', 2), ('CWH', 3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to illustrate the different sampling strategies\n",
    "\n",
    "@interact(i=Dropdown(description=\"First P.\", index=0, options=zip(param_names, range(13))), j=Dropdown(description=\"Second P.\", index=1, options=zip(param_names, range(13))), n=(10, 1000, 10), m=(3, 20, 1))\n",
    "def plot_scatter(i, j, n=100, m=5):\n",
    "\n",
    "    np.random.seed(30)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 6, figsize=(15, 5), width_ratios=[4, 1, 4, 1, 4, 1], height_ratios=[1, 4], sharex=\"col\", sharey=\"row\")\n",
    "    \n",
    "    for k, strategy in enumerate([uniform, grid, lhs]):\n",
    "\n",
    "        df = pd.DataFrame.from_records(strategy(lower[[i, j]], upper[[i, j]], n, m=m))\n",
    "        df.columns = (i, j)\n",
    "        \n",
    "        ax = axs[1, k*2]\n",
    "        ax.scatter(df[i], df[j], color=f\"C{k}\")\n",
    "        ax.set_xlabel(param_names[i])\n",
    "        ax.set_ylabel(param_names[j] if k == 0 else \" \")\n",
    "        \n",
    "        ax = axs[0, k*2]\n",
    "        ax.hist(df[i], color=f\"C{k}\")\n",
    "        ax.set_title(strategy.__name__.upper() + (\"*\" if strategy == grid else \"\"))\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        ax = axs[1, k*2 + 1]\n",
    "        ax.hist(df[j], color=f\"C{k}\", orientation=\"horizontal\")\n",
    "        ax.set_xticks([])\n",
    "\n",
    "        ax = axs[0, k*2+1]\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_xlim(ax.get_ylim())\n",
    "        \n",
    "    plt.tight_layout(w_pad=-0.5)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"* Due to performance reasons, the grid strategy samples n random points from a mxm grid and not all possible points.\")\n",
    "    print(\"  This will become handy later, when we sample for all 13 parameters. Then, the grid then would have m**13 points which are to many to run HBV for.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925db703-d2c2-4851-8e14-720387a94596",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task I: Parameter Distributions</h4>\n",
    "    Above you find the joint and marginal paramter distributions for three different sampling strategies (uniform, grid, LHS).\n",
    "    <ol>\n",
    "        <li>What differences and similarities between the joint and marginal distributions can you spot?</li>\n",
    "        <li>Which sampling strategy fill the parameter space best and why is this important?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61ac6c-0587-4837-84ea-929fe92c8c6d",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2925a-6124-4e2e-8680-99fe011725ef",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971a9c1-56d7-4b64-bb51-688171e7759a",
   "metadata": {},
   "source": [
    "## Evaluating HBV for Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e1f61b-51c9-4cfb-b4f6-11b5fa28a34a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2977c1f71ba47bfa55f542d71e80eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=500, description='n', max=1000, min=10, step=10), IntSlider(value=10, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to run hbv for sampled parameter values and plot results\n",
    "\n",
    "runs = dict()\n",
    "\n",
    "@interact(n=(10, 1000, 10), m=(3, 20, 1))\n",
    "def plot_metrics(n=500, m=10):\n",
    "\n",
    "    np.random.seed(30)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(4, 6), sharex=\"col\", sharey=True)\n",
    "    \n",
    "    for k, strategy in enumerate([uniform, grid, lhs]):\n",
    "        print(f\"Running HBV for n={n} samples for the {strategy.__name__.upper()} strategy\")\n",
    "        \n",
    "        # sample n parameters using the strategy\n",
    "        df = pd.DataFrame.from_records(strategy(lower, upper, n, m=m))\n",
    "\n",
    "        # evaluate hbv using rmse and nse\n",
    "        df_rmse = df.apply(lambda params: rmse(df_obs[\"Q [mm/day]\"], hbv(params.to_numpy(), df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])), axis=1)\n",
    "        df_nse  = df.apply(lambda params:  nse(df_obs[\"Q [mm/day]\"], hbv(params.to_numpy(), df_obs[\"P [mm/day]\"], df_obs[\"T [C]\"], df_obs[\"PET [mm/day]\"])), axis=1)\n",
    "        runs[(strategy, \"rmse\")] = pd.concat([df, df_rmse], axis=1)\n",
    "        runs[(strategy,  \"nse\")] = pd.concat([df,  df_nse], axis=1)\n",
    "        \n",
    "        ax = axs[k, 0]\n",
    "        ax.hist(df_rmse.values, color=f\"C{k}\", bins=30)\n",
    "        ax.set_xlabel(\"RMSE\")\n",
    "        ax.set_title(strategy.__name__.upper())\n",
    "        \n",
    "        ax = axs[k, 1]\n",
    "        ax.hist( df_nse.values, color=f\"C{k}\", bins=30)\n",
    "        ax.set_xlabel(\"NSE\")\n",
    "        ax.set_title(strategy.__name__.upper())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b5ec5-8d9d-4da4-97ae-51b34ec7bf2b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task II: Model Evaluation</h4>\n",
    "    You can now run HBV for n parameters using the three sampling strategies (m tunes the grid size). The resulting plots show the distributions of RMSE and NSE for these model runs.\n",
    "    <ol>\n",
    "        <li>Can you spot further similarities and differences between the strategies?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3d50c-0006-47cd-979d-9a265307fd82",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e1029-6c89-460f-b239-aebe47ed544b",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a5356-3694-452b-b08d-cb0e41511b52",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots and Parameter Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6a6b7f-cf30-4723-91c0-4fa6315fd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_df(df, n_groups=10):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        The function will be called automatically once for each paramter.\n",
    "        The input is a pandas DataFrame with columns (param, metric) holding the parmater values and metric values that each run produced.\n",
    "        These are the points in the scatter plots below.\n",
    "    Your Task:\n",
    "        Cut the param range into n_groups sections and calculate the mean parameter value and mean and variance of the metric value for each group.\n",
    "    Expected Return:\n",
    "        The resulting dataframe should have three columns (mean_param, mean_metric, vari_metric) and n_group rows.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def parameter_relevance(binned_df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        This function will be called once per paramter.\n",
    "        The input is your result from the bin_df function which represents the partial dependence of the metric on one parameter. \n",
    "        It should have columns (mean_param, mean_metric, vari_meric) and n_groups rows.\n",
    "    Your Task:\n",
    "        Think of and implement a way to measure which parameter is relevant, e.g. measure the impact of a paramter on the metric.\n",
    "    Expected Return:\n",
    "        A number that represents the paramter relevance.\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e97b251-9662-405d-b3d3-0beb6b4e4607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3515607c8c854f77926bfe8f005dc3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='metric', options=('rmse', 'nse'), value='rmse'), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT ALTER! code to plot partial dependence plots and parameter relevances\n",
    "\n",
    "relevances = dict()\n",
    "\n",
    "@interact(metric=[\"rmse\", \"nse\"])\n",
    "def plot_params_relation(metric):\n",
    "\n",
    "    params = np.array(param_names)[[1, 4, 7, 9, 11]]\n",
    "\n",
    "    if len(runs) == 0:\n",
    "        print(\"To make this plot, please run HBV in Task II at least once.\")\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 5, figsize=(5/3*6, 6), sharex=\"col\", sharey=\"row\")\n",
    "\n",
    "    for i, strategy in enumerate([uniform, grid, lhs]):\n",
    "        for j, param in enumerate(params):\n",
    "            \n",
    "            # grab the HBV runs from Task II\n",
    "            df = runs[(strategy, metric)]\n",
    "            m = df.iloc[:,-1] # the metric values\n",
    "            p = df.iloc[:,param_names.index(param)]  # the parameter values\n",
    "\n",
    "            # scatter all the runs in the background\n",
    "            ax = axs[i, j]\n",
    "            ax.scatter(p, m, color=f\"C{i}\", alpha=0.05)\n",
    "\n",
    "            # bin the values into groups (IMPLEMENTED BY STUDENTS)\n",
    "            df = pd.DataFrame(dict(param=p, metric=m))\n",
    "            df = bin_df(df, n_groups=10)\n",
    "\n",
    "            if df is not None:\n",
    "                # check for errors in the implementation\n",
    "                if df.shape != (10, 3):\n",
    "                    raise RuntimeWarning(f\"shape should be {(10, 3)}, yours has {df.shape}\")\n",
    "                if np.any(df.columns != [\"mean_param\", \"mean_metric\", \"vari_metric\"]):\n",
    "                    raise RuntimeWarnign(f\"columns should be {['mean_param', 'mean_metric', 'vari_metric']}, yours has {df.columns}\")\n",
    "\n",
    "                # plot binned values and confidence intervals\n",
    "                ax.scatter(df.mean_param, df.mean_metric, color=f\"C{i}\")\n",
    "                ax.plot([df.mean_param, df.mean_param], [df.mean_metric - df.vari_metric**0.5, df.mean_metric + df.vari_metric**0.5], color=f\"C{i}\", marker=\"_\", ms=5)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"please implement \\n bin_df\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                \n",
    "\n",
    "            # calculate parameter relevance (IMPLEMENTED BY STUDENTS)\n",
    "            relevance = parameter_relevance(df) if df is not None else np.nan\n",
    "            #if not isinstance(relevance, Number):\n",
    "            #    raise RuntimeWarning(f\"relevance should be a float number, yours is {type(relevance)})\")\n",
    "            relevances[(strategy.__name__, param)] = relevance\n",
    "\n",
    "            if j == 0: ax.set_title(strategy.__name__.upper())\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.set_xlabel(param)\n",
    "\n",
    "    fig.suptitle(f\"Partial Dependene of {metric.upper()} on Parameters\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5/3*6, 3), sharey=True)\n",
    "\n",
    "    for i, strategy in enumerate([uniform, grid, lhs]):\n",
    "        \n",
    "        df = pd.DataFrame(relevances.values(), index=relevances.keys()).unstack(sort=False)\n",
    "        df = df[df.index == strategy.__name__]\n",
    "        \n",
    "        ax = axs[i]\n",
    "        ax.bar(params, df.replace(np.nan, 0).values.flatten(), color=f\"C{i}\")\n",
    "        ax.set_title(strategy.__name__.upper())\n",
    "        ax.set_ylabel(\"Parameter Importance\")\n",
    "\n",
    "        if np.all(np.isnan(df.values.flatten())):\n",
    "            ax.text(0.5, 0.5, \"please implement\\nparameter_relevance\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "    fig.suptitle(\"Your Parameter Importances\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcd1a6-c627-41bd-abcf-74264b2b5810",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "    <h4><span>&#129300 </span>Task III: Partial Dependence Plots and Paramter Relevance</h4>\n",
    "    <p>\n",
    "        Now it's your turn to implement some code for yourself. Above, we want to show partial dependence plots of the metric values on the parameter and access paramter importance relevance in some way.\n",
    "        As you can see, we already scattered the RMSE and NSE values against the paramter values for all the HBV runs from Task II. Above the cell that creates this plot you will find two functions that you need to implement yourself.\n",
    "    </p>\n",
    "    <ol>\n",
    "        <li>Your first task is to implement the function <code>bin_df</code> that bins the parameter ranges and calculates mean and variance for the metric values in each bin. You'll find further information on the implementation details in the docstring.\n",
    "        <li>Once you have implemented your solution, rerun the plotting cell. What patterns can you now spot in the partial dependence plots?</li>\n",
    "    </ol>\n",
    "    <p>We now want to use these partial dependence plots to estimate parameter relevance in the model. The function <code>paramter_relevance</code> will be called with the dataframes that your implementation of <code>bin_df</code> returns and should return a number that represents the relevance of a parameter. For this task we wan't you to come up with your own way of determining which parameter is relevant. Don' worry - there is no one solution. </p>\n",
    "    <ol start=3>\n",
    "        <li>Implement your idea in the <code>parameter_relevance</code> function.</li>\n",
    "        <li>Once you have implemented your solution, rerun the plotting cell. Which parameters have a high relevance according to your method.</li>\n",
    "    </ol>\n",
    "    <p>When you struggle to implement the code, you can find hints and solutions below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f7874-8e66-4e74-a61a-93b9d4a61cbc",
   "metadata": {},
   "source": [
    "_PUT YOUR ANSWERS HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1a4d0-1f1c-41ef-bf7b-c01bde000cf0",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\"background:#e0f2fe; padding: 1%; border:1mm solid SkyBlue; color:black\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee82a4b-f8ae-41c4-b1e2-cb14a0333c13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hints and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d2642-9d8b-4c44-a6fc-5213452b8d79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hint for Dataframe Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5352ab-564c-4304-b16a-a39fbb0560f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Pandas offers easy dataframe manipulation. To functions that could be helpful are `df.groupby` and `pd.cut`. You can read more about them in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db441fdb-2b48-4225-b50f-bb836dec7e80",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe10b3d-4f5f-48de-8c06-d9245955b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_df(df, n_groups=10):\n",
    "    \"\"\"\n",
    "    Input: df is a pandas DataFrame with columns (param, metric) holding the metric values.\n",
    "    Your task is to to cut the param into n_groups sections and calculate the mean parameter value and mean and variance of the metric value. \n",
    "    The resulting dataframe should have three columns (mean_param, mean_metric, vari_metric).\n",
    "    \"\"\"\n",
    "    groups = pd.cut(df.param, n_groups)\n",
    "    binned_df = pd.DataFrame()\n",
    "    binned_df[\"mean_param\"]  = df.groupby(groups, observed=False).mean().param.values\n",
    "    binned_df[\"mean_metric\"] = df.groupby(groups, observed=False).mean().metric.values\n",
    "    binned_df[\"vari_metric\"] = df.groupby(groups, observed=False).var().metric.values\n",
    "    return binned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef960c87-675a-4d2d-85c2-1571baf397c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hint for Parameter Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b54dc3-dba0-48b8-a692-d7aa34451d57",
   "metadata": {},
   "source": [
    "Note how for some paramters the RMSE or NSE is nearly independent of the parameter value. How could you express the strength of the relationship between the parameter value and the metric value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328e69c-2230-44b6-b0fe-3d76ba4c2f3e",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace45d1c-be34-4749-a5c4-7b9e80e4cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_relevance(binned_df):\n",
    "    # variance of the mean metric values -> higher trend means higher variance\n",
    "    return binned_df.mean_metric.var()\n",
    "    # another approach could be to calculate the regression coefficient\n",
    "    # return abs(np.corrcoef(binned_df.mean_param, binned_df.vari_metric)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa510d5b-c5f1-451f-8c9e-dcdc23cdf753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
